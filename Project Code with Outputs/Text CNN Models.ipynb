{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Code for evaluating the best Deep Learning Models for Text Classification"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nplt.style.use('ggplot')\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nproject_text_tokens_forLSTMBagging = pd.read_csv(\"/kaggle/input/bilstmbagging/project-text-tokens-for-LSTM-Bagging.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"project_text_tokens_forLSTMBagging['tokens'] = project_text_tokens_forLSTMBagging['tokens'].apply(lambda x: eval(x))\nproject_text_tokens_forLSTMBagging.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_list = list(project_text_tokens_forLSTMBagging.tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import Input, Model\nfrom keras.layers import Embedding, GlobalAveragePooling1D, Dense\nclass FastText(object):\n    def __init__(self, maxlen, max_features, embedding_dims,\n                 class_num=1,\n                 last_activation='sigmoid'):\n        self.maxlen = maxlen\n        self.max_features = max_features\n        self.embedding_dims = embedding_dims\n        self.class_num = class_num\n        self.last_activation = last_activation\n\n    def get_model(self):\n        input = Input((self.maxlen,))\n\n        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n        x = GlobalAveragePooling1D()(embedding)\n\n        output = Dense(self.class_num, activation=self.last_activation)(x)\n        model = Model(inputs=input, outputs=output)\n        return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing import sequence\ndef create_ngram_set(input_list, ngram_value=2):\n    \"\"\"\n    Extract a set of n-grams from a list of integers.\n    # >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n    {(4, 9), (4, 1), (1, 4), (9, 4)}\n    # >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n    \"\"\"\n    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\ndef add_ngram(sequences, token_indice, ngram_range=2):\n    \"\"\"\n    Augment the input list of list (sequences) by appending n-grams values.\n    Example: adding bi-gram\n    # >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n    # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n    # >>> add_ngram(sequences, token_indice, ngram_range=2)\n    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n    Example: adding tri-gram\n    # >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n    # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n    # >>> add_ngram(sequences, token_indice, ngram_range=3)\n    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n    \"\"\"\n    new_sequences = []\n    for input_list in sequences:\n        new_list = input_list[:]\n        for ngram_value in range(2, ngram_range + 1):\n            for i in range(len(new_list) - ngram_value + 1):\n                ngram = tuple(new_list[i:i + ngram_value])\n                if ngram in token_indice:\n                    new_list.append(token_indice[ngram])\n        new_sequences.append(new_list)\n    return new_sequences\n# Set parameters:\n# ngram_range = 2 will add bi-grams features\nngram_range = 1\nmax_features = 40000\nmaxlen = 240\nbatch_size = 32\nembedding_dims = 60\nepochs = 5\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(token_list)\nproj_copy = project_text_tokens_forLSTMBagging.copy()\ntrain_set = proj_copy.sample(frac=0.8, random_state=2)\ntest_set = proj_copy.drop(train_set.index) \nX_train = tokenizer.texts_to_sequences(train_set.tokens)\nX_test = tokenizer.texts_to_sequences(test_set.tokens)\ny_train = train_set.state\ny_test = test_set.state\nprint('Loading data...')\nif ngram_range > 1:\n    print('Adding {}-gram features'.format(ngram_range))\n    # Create set of unique n-gram from the training set.\n    ngram_set = set()\n    for input_list in X_train:\n        for i in range(2, ngram_range + 1):\n            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n            ngram_set.update(set_of_ngram)\n\n    # Dictionary mapping n-gram token to a unique integer.\n    # Integer values are greater than max_features in order\n    # to avoid collision with existing features.\n    start_index = max_features + 1\n    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n    indice_token = {token_indice[k]: k for k in token_indice}\n\n    # max_features is the highest integer that could be found in the dataset.\n    max_features = np.max(list(indice_token.keys())) + 1\n\n    # Augmenting x_train and x_test with n-grams features\n    X_train = add_ngram(X_train, token_indice, ngram_range)\n    X_test = add_ngram(X_test, token_indice, ngram_range)\n    print('Average train sequence length: {}'.format(np.mean(list(map(len, X_train)), dtype=int)))\n    print('Average test sequence length: {}'.format(np.mean(list(map(len, X_test)), dtype=int)))\n\nprint('Pad sequences (samples x time)...')\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\nprint('x_train shape:', X_train.shape)\nprint('x_test shape:', X_test.shape)\n\nprint('Build model...')\nmodel = FastText(maxlen, max_features, embedding_dims,1,'sigmoid').get_model()\nmodel.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\nmodel.summary()\nprint('Train...')\nearly_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\nhistory = model.fit(X_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          callbacks=[early_stopping],\n          validation_data=(X_test, y_test),\n          verbose=1)\n\nprint('Test...')\nresult = model.predict(X_test)\nprint(result)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import Input, Model\nfrom keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\n\nclass TextCNN(object):\n    def __init__(self, maxlen, max_features, embedding_dims,\n                 class_num=1,\n                 last_activation='sigmoid'):\n        self.maxlen = maxlen\n        self.max_features = max_features\n        self.embedding_dims = embedding_dims\n        self.class_num = class_num\n        self.last_activation = last_activation\n\n    def get_model(self):\n        input = Input((self.maxlen,))\n\n        # Embedding part can try multichannel as same as origin paper\n        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n        convs = []\n        for kernel_size in [3, 4, 5]:\n            c = Conv1D(128, kernel_size, activation='relu')(embedding)\n            c = GlobalMaxPooling1D()(c)\n            convs.append(c)\n        x = Concatenate()(convs)\n\n        output = Dense(self.class_num, activation=self.last_activation)(x)\n        model = Model(inputs=input, outputs=output)\n        return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nfrom keras.preprocessing import sequence\n\nmax_features = 32000\nmaxlen = 240\nbatch_size = 20\nembedding_dims = 360\nepochs = 2\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(token_list)\nproj_copy = project_text_tokens_forLSTMBagging.copy()\ntrain_set = proj_copy.sample(frac=0.8, random_state=2)\ntest_set = proj_copy.drop(train_set.index) \nx_train = tokenizer.texts_to_sequences(train_set.tokens)\nx_test = tokenizer.texts_to_sequences(test_set.tokens)\ny_train = train_set.state\ny_test = test_set.state\n\nprint('Pad sequences (samples x time)...')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\nprint('Build model...')\nmodel2 = TextCNN(maxlen, max_features, embedding_dims).get_model()\nmodel2.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\nmodel2.summary()\n\nprint('Train...')\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max')\nhistory = model2.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          callbacks=[early_stopping],\n          validation_data=(x_test, y_test))\n\nprint('Test...')\nresult2 = model2.predict(x_test)\nprint(result2)\nloss, accuracy = model2.evaluate(x_train, y_train, verbose=True)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model2.evaluate(x_test, y_test, verbose=True)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import Model\nfrom keras.layers import Embedding, Dense, LSTM\n\nclass TextRNN(Model):\n    def __init__(self,\n                 maxlen,\n                 max_features,\n                 embedding_dims,\n                 class_num=1,\n                 last_activation='sigmoid'):\n        super(TextRNN, self).__init__()\n        self.maxlen = maxlen\n        self.max_features = max_features\n        self.embedding_dims = embedding_dims\n        self.class_num = class_num\n        self.last_activation = last_activation\n        self.embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n        self.rnn = LSTM(128)  # LSTM or GRU\n        self.classifier = Dense(self.class_num, activation=self.last_activation)\n\n    def call(self, inputs):\n        if len(inputs.get_shape()) != 2:\n            raise ValueError('The rank of inputs of TextRNN must be 2, but now is %d' % len(inputs.get_shape()))\n        if inputs.get_shape()[1] != self.maxlen:\n            raise ValueError('The maxlen of inputs of TextRNN must be %d, but now is %d' % (self.maxlen, inputs.get_shape()[1]))\n        embedding = self.embedding(inputs)\n        x = self.rnn(embedding)\n        output = self.classifier(x)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing import sequence\n\nmax_features = 30000\nmaxlen = 200\nbatch_size = 64\nembedding_dims = 60\nepochs = 2\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(token_list)\nproj_copy = project_text_tokens_forLSTMBagging.copy()\ntrain_set = proj_copy.sample(frac=0.8, random_state=2)\ntest_set = proj_copy.drop(train_set.index) \nx_train = tokenizer.texts_to_sequences(train_set.tokens)\nx_test = tokenizer.texts_to_sequences(test_set.tokens)\ny_train = train_set.state\ny_test = test_set.state\n\nprint('Pad sequences (samples x time)...')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\nprint('y_test shape:', y_test.shape)\npass\nprint('Build model...')\nmodel3 = TextRNN(maxlen, max_features, embedding_dims)\nmodel3.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\nprint('Train...')\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max')\nmodel3.fit(x_train, y_train.values,\n          batch_size=batch_size,\n          epochs=epochs,\n          callbacks=[early_stopping],\n          validation_data=(x_test,y_test.values))\nprint(model3.summary())\nprint('Test...')\nresult3 = model3.predict(x_test)\nprint(result3)\nloss, accuracy = model3.evaluate(x_train, y_train.values, verbose=True)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model3.evaluate(x_test, y_test.values, verbose=True)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import Model\nfrom tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional\n\nclass TextBiRNN(Model):\n    def __init__(self,\n                 maxlen,\n                 max_features,\n                 embedding_dims,\n                 class_num=1,\n                 last_activation='sigmoid'):\n        super(TextBiRNN, self).__init__()\n        self.maxlen = maxlen\n        self.max_features = max_features\n        self.embedding_dims = embedding_dims\n        self.class_num = class_num\n        self.last_activation = last_activation\n        self.embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n        self.bi_rnn = Bidirectional(LSTM(128))  # LSTM or GRU\n        self.classifier = Dense(self.class_num, activation=self.last_activation)\n\n    def call(self, inputs):\n        if len(inputs.get_shape()) != 2:\n            raise ValueError('The rank of inputs of TextBiRNN must be 2, but now is %d' % len(inputs.get_shape()))\n        if inputs.get_shape()[1] != self.maxlen:\n            raise ValueError('The maxlen of inputs of TextBiRNN must be %d, but now is %d' % (self.maxlen, inputs.get_shape()[1]))\n        embedding = self.embedding(inputs)\n        x = self.bi_rnn(embedding)\n        output = self.classifier(x)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing import sequence\n\nmax_features = 40000\nmaxlen = 220\nbatch_size = 40\nembedding_dims = 60\nepochs = 2\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(token_list)\nproj_copy = project_text_tokens_forLSTMBagging.copy()\ntrain_set = proj_copy.sample(frac=0.8, random_state=2)\ntest_set = proj_copy.drop(train_set.index) \nx_train = tokenizer.texts_to_sequences(train_set.tokens)\nx_test = tokenizer.texts_to_sequences(test_set.tokens)\ny_train = train_set.state\ny_test = test_set.state\n\nprint('Pad sequences (samples x time)...')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\nprint('y_test shape:', y_test.shape)\npass\nprint('Build model...')\nmodel4 = TextBiRNN(maxlen, max_features, embedding_dims)\nmodel4.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\nprint('Train...')\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max')\nmodel4.fit(x_train, y_train.values,\n          batch_size=batch_size,\n          epochs=epochs,\n          callbacks=[early_stopping],\n          validation_data=(x_test,y_test.values))\nprint(model3.summary())\nprint('Test...')\nresult4 = model4.predict(x_test)\nprint(result4)\nloss, accuracy = model4.evaluate(x_train, y_train.values, verbose=True)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model4.evaluate(x_test, y_test.values, verbose=True)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model4.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Embedding, Dense, SimpleRNN, Lambda, Concatenate, Conv1D, GlobalMaxPooling1D\n\n\nclass RCNN(Model):\n    def __init__(self,\n                 maxlen,\n                 max_features,\n                 embedding_dims,\n                 class_num=1,\n                 last_activation='sigmoid'):\n        super(RCNN, self).__init__()\n        self.maxlen = maxlen\n        self.max_features = max_features\n        self.embedding_dims = embedding_dims\n        self.class_num = class_num\n        self.last_activation = last_activation\n        self.embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n        self.forward_rnn = SimpleRNN(128, return_sequences=True)\n        self.backward_rnn = SimpleRNN(128, return_sequences=True, go_backwards=True)\n        self.reverse = Lambda(lambda x: tf.reverse(x, axis=[1]))\n        self.concatenate = Concatenate(axis=2)\n        self.conv = Conv1D(64, kernel_size=1, activation='tanh')\n        self.max_pooling = GlobalMaxPooling1D()\n        self.classifier = Dense(self.class_num, activation=self.last_activation)\n\n    def call(self, inputs):\n        if len(inputs) != 3:\n            raise ValueError('The length of inputs of RCNN must be 3, but now is %d' % len(inputs))\n        input_current = inputs[0]\n        input_left = inputs[1]\n        input_right = inputs[2]\n        if len(input_current.get_shape()) != 2 or len(input_left.get_shape()) != 2 or len(input_right.get_shape()) != 2:\n            raise ValueError('The rank of inputs of RCNN must be (2, 2, 2), but now is (%d, %d, %d)' % (len(input_current.get_shape()), len(input_left.get_shape()), len(input_right.get_shape())))\n        if input_current.get_shape()[1] != self.maxlen or input_left.get_shape()[1] != self.maxlen or input_right.get_shape()[1] != self.maxlen:\n            raise ValueError('The maxlen of inputs of RCNN must be (%d, %d, %d), but now is (%d, %d, %d)' % (self.maxlen, self.maxlen, self.maxlen, input_current.get_shape()[1], input_left.get_shape()[1], input_right.get_shape()[1]))\n        embedding_current = self.embedding(input_current)\n        embedding_left = self.embedding(input_left)\n        embedding_right = self.embedding(input_right)\n        x_left = self.forward_rnn(embedding_left)\n        x_right = self.backward_rnn(embedding_right)\n        x_right = self.reverse(x_right)\n        x = self.concatenate([x_left, embedding_current, x_right])\n        x = self.conv(x)\n        x = self.max_pooling(x)\n        output = self.classifier(x)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing import sequence\n\nmax_features = 32000\nmaxlen = 240\nbatch_size = 20\nembedding_dims = 360\nepochs = 3\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(token_list)\nproj_copy = project_text_tokens_forLSTMBagging.copy()\ntrain_set = proj_copy.sample(frac=0.8, random_state=2)\ntest_set = proj_copy.drop(train_set.index) \nx_train = tokenizer.texts_to_sequences(train_set.tokens)\nx_test = tokenizer.texts_to_sequences(test_set.tokens)\ny_train = train_set.state.values\ny_test = test_set.state.values\nprint('Loading data...')\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\nprint('Pad sequences (samples x time)...')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\nprint('Prepare input for model...')\nx_train_current = x_train\nx_train_left = np.hstack([np.expand_dims(x_train[:, 0], axis=1), x_train[:, 0:-1]])\nx_train_right = np.hstack([x_train[:, 1:], np.expand_dims(x_train[:, -1], axis=1)])\nx_test_current = x_test\nx_test_left = np.hstack([np.expand_dims(x_test[:, 0], axis=1), x_test[:, 0:-1]])\nx_test_right = np.hstack([x_test[:, 1:], np.expand_dims(x_test[:, -1], axis=1)])\nprint('x_train_current shape:', x_train_current.shape)\nprint('x_train_left shape:', x_train_left.shape)\nprint('x_train_right shape:', x_train_right.shape)\nprint('x_test_current shape:', x_test_current.shape)\nprint('x_test_left shape:', x_test_left.shape)\nprint('x_test_right shape:', x_test_right.shape)\nprint('Build model...')\nmodel5 = RCNN(maxlen, max_features, embedding_dims)\nmodel5.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\nprint('Train...')\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max')\nmodel5.fit([x_train_current, x_train_left, x_train_right], y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          callbacks=[early_stopping],\n          validation_data=([x_test_current, x_test_left, x_test_right], y_test))\nprint(model5.summary())\nprint('Test...')\nresult5 = model5.predict([x_test_current, x_test_left, x_test_right])\nprint(result5)\nloss, accuracy = model5.evaluate([x_train_current, x_train_left, x_train_right], y_train, verbose=True)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model5.evaluate([x_test_current, x_test_left, x_test_right], y_test, verbose=True)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outcome = pd.DataFrame({'id': list(test_set.project_id), 'state': list(result.flatten())})\noutcome.to_csv('fast-text-res.csv',index=False)\noutcome.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outcome2 = pd.DataFrame({'id': list(test_set.project_id), 'state': list(result2.flatten())})\noutcome2.to_csv('text-cnn-res.csv',index=False)\noutcome2.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outcome3 = pd.DataFrame({'id': list(test_set.project_id), 'state': list(result3.flatten())})\noutcome3.to_csv('text-rnn-res.csv',index=False)\noutcome3.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outcome4 = pd.DataFrame({'id': list(test_set.project_id), 'state': list(result4.flatten())})\noutcome4.to_csv('text-birnn-res.csv',index=False)\noutcome4.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outcome5 = pd.DataFrame({'id': list(test_set.project_id), 'state': list(result5.flatten())})\noutcome5.to_csv('text-rcnn-res.csv',index=False)\noutcome5.head(5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}